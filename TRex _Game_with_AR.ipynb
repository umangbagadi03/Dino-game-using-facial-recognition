{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caca647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogui in c:\\users\\umang\\anaconda3\\lib\\site-packages (0.9.53)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in c:\\users\\umang\\anaconda3\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: PyTweening>=1.0.1 in c:\\users\\umang\\anaconda3\\lib\\site-packages (from pyautogui) (1.0.4)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in c:\\users\\umang\\anaconda3\\lib\\site-packages (from pyautogui) (0.1.28)\n",
      "Requirement already satisfied: pymsgbox in c:\\users\\umang\\anaconda3\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: mouseinfo in c:\\users\\umang\\anaconda3\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pyrect in c:\\users\\umang\\anaconda3\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\umang\\anaconda3\\lib\\site-packages (from mouseinfo->pyautogui) (1.8.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519b50dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python==4.6.0.66 in c:\\users\\umang\\anaconda3\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\umang\\anaconda3\\lib\\site-packages (from opencv-python==4.6.0.66) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python==4.6.0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df3debdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in c:\\users\\umang\\anaconda3\\lib\\site-packages (3.24.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cmake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64f8124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dlib in c:\\users\\umang\\anaconda3\\lib\\site-packages (19.24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903665f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import hypot\n",
    "import pyautogui\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5a38c",
   "metadata": {},
   "source": [
    "Step 1: Real-time Face Detection\n",
    "\n",
    "Step 2: Find the landmarks for the detected face\n",
    "\n",
    "Step 3: Build the Jump Control mechanism for the Dino\n",
    "\n",
    "Step 4: Build the Crouch Control mechanism\n",
    "\n",
    "Step 5: Perform Calibration\n",
    "\n",
    "Step 6: Keyboard Automation with PyautoGUI\n",
    "\n",
    "Step 7: Build the Final Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523e201",
   "metadata": {},
   "source": [
    "## 1: Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d0407",
   "metadata": {},
   "source": [
    "We can use OpenCV’s Haar cascades or Dlib’s HOG-based face detector but instead, we are going to use a more robust deep learning-based SSD face detector with OpenCV’s DNN module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4b2a0",
   "metadata": {},
   "source": [
    "Initialize The DNN Module:\n",
    "\n",
    "The SSD face detector provided by OpenCV is a Caffe model and you will need two files to do inference with it. A .prototxt file that defines the model architecture and a .caffemodel file that contains the pre-trained weights for the layers in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad431ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e77c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for path in ['deploy.prototxt.txt', 'res10_300x300_ssd_iter_140000_fp16.caffemodel']:\n",
    "    print(os.path.exists(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d9d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required file path\n",
    "model_weights =  \"dependencies/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "\n",
    "# architecture file path\n",
    "model_arch = \"dependencies/deploy.prototxt.txt\"\n",
    "\n",
    "# Load the caffe model\n",
    "net = cv2.dnn.readNetFromCaffe(model_arch, model_weights)\n",
    "#net = cv2.dnn.readNetFromCaffe( model_weights,model_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58b7a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(image, threshold =0.7):   # In thresholding, we convert an image from colour or grayscale into a binary image\n",
    "    \n",
    "    # Get the height,width of the image\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Apply mean subtraction, and create 4D blob from image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0,(300, 300), (104.0, 117.0, 123.0))\n",
    "    \n",
    "    # Set the new input value for the network\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Run forward pass on the input to compute output\n",
    "    faces = net.forward()\n",
    "    \n",
    "    # Get the confidence value for all detected faces\n",
    "    prediction_scores = faces[:,:,:,2]\n",
    "    \n",
    "    # Get the index of the prediction with highest confidence \n",
    "    i = np.argmax(prediction_scores)\n",
    "    \n",
    "    # Get the face with highest confidence \n",
    "    face = faces[0,0,i]\n",
    "    \n",
    "    # Extract the confidence\n",
    "    confidence = face[2]\n",
    "    \n",
    "    # if confidence value is greater than the threshold\n",
    "    if confidence > threshold:   # will conside face with highest confidence as we only want single face\n",
    "        \n",
    "        # The 4 values at indexes 3 to 6 are the top-left, bottom-right coordinates\n",
    "        # scales to range 0-1.The original coordinates can be found by \n",
    "        # multiplying x,y values with the width,height of the image\n",
    "        box = face[3:7] * np.array([w, h, w, h])\n",
    "        \n",
    "        # The coordinates are the pixel numbers relative to the top left\n",
    "        # corner of the image therefore needs be quantized to int type\n",
    "        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "        \n",
    "        # Draw a bounding box around the face.\n",
    "        annotated_frame = cv2.rectangle(image.copy(), (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        output = (annotated_frame, (x1, y1, x2, y2), True, confidence)\n",
    "    \n",
    "    # Return the original frame if no face is detected with high confidence.\n",
    "    else:\n",
    "        output = (image,(),False, 0)\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0019f4b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFace Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, annotated_frame)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Break the loop if ESC key pressed\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# When everything done, release the capture and destroy the window\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)   # Create a video capture object, in this case we are reading the video from a file\n",
    "\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Face Detection', cv2.WINDOW_NORMAL) \n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    annotated_frame, coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Detection', annotated_frame)\n",
    "    \n",
    "    # Break the loop if ESC key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e8b23",
   "metadata": {},
   "source": [
    "## Step 2: Landmarks Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4154de",
   "metadata": {},
   "source": [
    "To implement the jump mechanism, we need information about whether the mouth is open or closed. For this, we need to detect facial landmarks so we can determine the position of upper and lower lips.\n",
    "\n",
    "By using Dlib’s 68 landmark detector, we’ll be able to detect below 68 landmarks on the face.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4bf574",
   "metadata": {},
   "source": [
    "Download model from here - https://github.com/italojs/facial-landmarks-recognition/blob/master/shape_predictor_68_face_landmarks.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e3c0a",
   "metadata": {},
   "source": [
    "To initialize the landmark detector, you will use dlib.shape_predictor() function, which will load the pre-trained landmark detector from the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the landmark detector\n",
    "\n",
    "predictor = dlib.shape_predictor(\"dependencies/shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3102c",
   "metadata": {},
   "source": [
    "#### Create the detect_landmarks() function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa24fe7",
   "metadata": {},
   "source": [
    "Now we will create a function called detect_landmarks() which takes in the coordinates of the detected face returned by the face_detector() method and then detects landmarks and returns them, while also annotating the image with circles on landmark positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec0b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmarks(box, image):\n",
    "    \n",
    "    # For faster results convert the image to gray-scale\n",
    "    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get the coordinates \n",
    "    (x1, y1, x2, y2) = box\n",
    "\n",
    "    # Perform the detection\n",
    "    shape = predictor(gray_scale, dlib.rectangle(x1, y1, x2, y2))\n",
    "    \n",
    "    # Get the numPy array containing the coordinates of the landmarks\n",
    "    landmarks = shape_to_np(shape)\n",
    "    \n",
    "   # Draw the landmark points with circles \n",
    "    for (x, y) in landmarks:\n",
    "        annotated_image = cv2.circle(image, (x, y),2, (0, 127, 255), -1)\n",
    "\n",
    "    return annotated_image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037e66d",
   "metadata": {},
   "source": [
    "The helper function below converts the shape object returned by the predictor function into a more convenient NumPy array. So the helper function below is being used by the landmark function we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ace320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape): \n",
    "    \n",
    "    # Create an array of shape (68, 2) for storing the landmark coordinates\n",
    "    landmarks = np.zeros((68, 2), dtype=\"int\")\n",
    "    \n",
    "    # Write the x,y coordinates of each landmark into the array\n",
    "    for i in range(0, 68): \n",
    "        landmarks[i] = (shape.part(i).x, shape.part(i).y)\n",
    "        \n",
    "        \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1fd19",
   "metadata": {},
   "source": [
    "Test detect_landmarks() function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Landmark Detection', cv2.WINDOW_NORMAL) \n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Get the landmarks for the face region in the frame\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Landmark Detection',landmark_image)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d3b29",
   "metadata": {},
   "source": [
    "## Step 3: Jump Control mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb4903",
   "metadata": {},
   "source": [
    "we will start implementing the jump control mechanism, what we need to do is to make the dino jump when the mouth is opened and stay still when it’s not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5859c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mouth_open(landmarks, ar_threshold = 0.7): \n",
    "    \n",
    "    \n",
    "    # Calculate the euclidean distance labelled as A,B,C - FOR MOUTH OPEN / CLOSE\n",
    "    A = hypot(landmarks[50][0] - landmarks[58][0], landmarks[50][1] - landmarks[58][1])\n",
    "    B = hypot(landmarks[52][0] - landmarks[56][0], landmarks[52][1] - landmarks[56][1])\n",
    "    C = hypot(landmarks[48][0] - landmarks[54][0], landmarks[48][1] - landmarks[54][1])\n",
    "    \n",
    "    # Calculate the mouth aspect ratio\n",
    "    # The value of vertical distance A,B is averaged\n",
    "    mouth_aspect_ratio = (A + B) / (2.0 * C)\n",
    "    \n",
    "    # Return True if the value is greater than the threshold\n",
    "    if mouth_aspect_ratio > ar_threshold:\n",
    "        return True, mouth_aspect_ratio\n",
    "    else:\n",
    "        return False, mouth_aspect_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2bc63",
   "metadata": {},
   "source": [
    "Testing the is_mouth_open() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Mouth Status', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Get the landmarks for the face region in the frame\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Adjust the threshold and make sure it's working for you.\n",
    "        mouth_status,_ = is_mouth_open(landmarks, ar_threshold = 0.6)\n",
    "        \n",
    "        # Display the mouth status\n",
    "        cv2.putText(frame,'Is Mouth Open: {}'.format(mouth_status),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Mouth Status',frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "        \n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c80e3",
   "metadata": {},
   "source": [
    "## Step 4: Crouch Control Mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57927b9d",
   "metadata": {},
   "source": [
    "The Crouch control mechanism will utilize the euclidean distance between the top-left corner and bottom-right corner of the face bounding box. When the face is near the camera the distance will be greater, and when the face is close enough a key down event will be triggered causing the Dino to crouch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a40bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_proximity(box,image, proximity_threshold = 325):\n",
    "    \n",
    "    # Get the height and width of the face bounding box\n",
    "    face_width =  box[2]-box[0]\n",
    "    face_height = box[3]-box[1]\n",
    "    \n",
    "    # Draw rectangle to guide the user \n",
    "    # Calculate the angle of diagonal using face width, height \n",
    "    theta = np.arctan(face_height/face_width)\n",
    "     \n",
    "    # Use the angle to calculate height, width of the guide rectangle\n",
    "    guide_height = np.sin(theta)*proximity_threshold\n",
    "    guide_width  = np.cos(theta)*proximity_threshold\n",
    "    \n",
    "    # Calculate the mid-point of the guide rectangle/face bounding box\n",
    "    mid_x,mid_y = (box[2]+box[0])/2 , (box[3]+box[1])/2\n",
    "    \n",
    "    #Calculate to coordinates of top-left and bottom-right corners\n",
    "    guide_topleft = int(mid_x-(guide_width/2)), int(mid_y-(guide_height/2))\n",
    "    guide_bottomright = int(mid_x +(guide_width/2)), int(mid_y + (guide_height/2))\n",
    "    \n",
    "    # Draw the guide rectangle\n",
    "    cv2.rectangle(image, guide_topleft, guide_bottomright, (0, 255, 255), 2)\n",
    "    \n",
    "    # Calculate the diagonal distance of the bounding box\n",
    "    diagonal = hypot(face_width, face_height)\n",
    "    \n",
    "    # Return True if distance greater than the threshold\n",
    "    if diagonal > proximity_threshold:\n",
    "        return True, diagonal\n",
    "    else:\n",
    "        return False, diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362bc7b7",
   "metadata": {},
   "source": [
    "Testing the face_proximity() function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe268fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Face proximity', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Check if face is closer than the defined threshold\n",
    "        is_face_close,_ = face_proximity(box_coords, face_image, proximity_threshold = 325)\n",
    "        \n",
    "        # Display the mouth status\n",
    "        cv2.putText(face_image,'Is Face Close: {}'.format(is_face_close),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "\n",
    "        \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face proximity',face_image)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58fc7c",
   "metadata": {},
   "source": [
    "## Step 5: Perform Calibration (Optional)\n",
    "The threshold values I have selected for is_mouth_open() and face_proximity() should generalize well for most people but in case those values are not working for you, you can change it here.\n",
    "\n",
    "So in this step, we’ll learn how to Calibrate the ideal threshold values. Now run the code below and open-close your mouth and find the threshold that works best to differentiate between the two states. Also, do the same thing for the face proximity threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038fa491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Calibration', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Detect landmarks if the frame is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Get the current mouth aspect ratio\n",
    "        _,mouth_ar = is_mouth_open(landmarks)\n",
    "    \n",
    "        # Get the current face proximity\n",
    "        _, proximity  = face_proximity(box_coords, face_image)\n",
    "\n",
    "        # Calculate threshold values\n",
    "        ar_threshold = mouth_ar*1.4\n",
    "        proximity_threshold = proximity*1.3\n",
    "\n",
    "        \n",
    "        # Dsiplay the threshold values\n",
    "        cv2.putText(frame, 'Aspect ratio threshold: {:.2f} '.format(ar_threshold), \n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        \n",
    "        cv2.putText(frame,'Proximity threshold: {:.2f}'.format(proximity_threshold), \n",
    "                    (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "     \n",
    "    # Display the frame\n",
    "    cv2.imshow('Calibration',frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c4c91",
   "metadata": {},
   "source": [
    "## Step 6: Keyboard Automation\n",
    "Note: When running the following cells in Jupyter Notebook, make sure you don't use the Shift + Enter command to run the following code cells. You can use the Run code cell button in the Toolbar. Also if the keyboard buttons misbehave then you can also restart the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e9c6e",
   "metadata": {},
   "source": [
    "The Dino cannot jump or crouch unless it is provided with input from the keyboard. We will use the output from the is_mouth_open() and face_proximity() functions to trigger keyboard keypress events.\n",
    "\n",
    "For this, we will be using PyAutoGUI which is a simple API that lets a python script control the mouse and keyboard, it’s used to automate processes. Let’s have a look at how it works and how you can use it to trigger key events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fce373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will open a context menu\n",
    "pyautogui.click(button='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b955e9",
   "metadata": {},
   "source": [
    "To trigger key presses we can use press() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229039e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press space bar. This will scroll down the page in some browsers\n",
    "pyautogui.press('space')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09133c",
   "metadata": {},
   "source": [
    "To press multiple keys we can pass a list of strings to press() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will move the focus to the next cell in the notebook\n",
    "pyautogui.press(['shift','enter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0497e67",
   "metadata": {},
   "source": [
    "\n",
    "When you use pyautogui.keyDown() instead of pyautogui.press(). Then the specified key is held down unless pyautogui.keyUp()event takes place helping us trigger a longer key press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold down the shift key\n",
    "pyautogui.keyDown('shift')\n",
    "# Press enter while the shift key is down, this will run the next code cell\n",
    "pyautogui.press('enter')\n",
    "\n",
    "# Release the shift key\n",
    "pyautogui.keyUp('shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run automatically after running the two code cells above\n",
    "print('I ran')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c110e",
   "metadata": {},
   "source": [
    "## Step 7: Build The Final Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48aae5",
   "metadata": {},
   "source": [
    "Go to Chrome://Dino in your Chrome browser and run the code cell below.\n",
    "\n",
    "Note: The image window screen will freeze when you trigger key buttons since at that moment the while loop will pause to press the key. So don't worry about that, after the program launches minimize the camera window and just go to chrome://dino/ and start playing using your face and mouth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Lets Go! Play the Game', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# By default each key press is followed by a 0.1 second pause\n",
    "pyautogui.PAUSE = 0.0\n",
    "\n",
    "# The fail-safe triggers an exception in case mouse\n",
    "# is moved to corner of the screen\n",
    "#pyautogui.FAILSAFE = False\n",
    "\n",
    "while(True):\n",
    "    \n",
    "     # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Detect landmarks if a face is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Check if mouth is open\n",
    "        is_open,_ = is_mouth_open(landmarks, ar_threshold)\n",
    "        \n",
    "        # If the mouth is open trigger space key Down event to jump\n",
    "        if is_open:\n",
    "            \n",
    "            pyautogui.keyDown('space')\n",
    "            mouth_status = 'Open'\n",
    "\n",
    "        else:\n",
    "            # Else the space key is Up\n",
    "            pyautogui.keyUp('space')\n",
    "            mouth_status = 'Closed'\n",
    "        \n",
    "        # Display the mouth status on the frame\n",
    "        cv2.putText(frame,'Mouth: {}'.format(mouth_status),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        \n",
    "        # Check the proximity of the face\n",
    "        is_closer,_  = face_proximity(box_coords, frame, proximity_threshold)\n",
    "        \n",
    "        # If face is closer press the down key\n",
    "        if is_closer:\n",
    "            pyautogui.keyDown('down')\n",
    "            \n",
    "        else:\n",
    "            pyautogui.keyUp('down')\n",
    "        \n",
    "    # Display the frame\n",
    "    cv2.imshow('Dino with OpenCV',frame)\n",
    "\n",
    "    # Break the loop if ESC key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34616424",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
